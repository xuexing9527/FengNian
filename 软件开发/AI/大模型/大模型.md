# 大模型

大模型让 AI 走进了公众视野，也让“AI”变得火热

神经网络模型

数十亿，上万亿参数的规模上训练的模型

特点：
- 超大规模参数
- 基于大量数据训练
- 多任务泛化能力
- 自监督学习


算力/数据与合规：训练与推断成本高，数据版权/隐私/偏见治理仍是工程挑战。

## GPT - AI 边界讨论
硬边界（深度+广度）

训练目标限制：最大似然 ≠ 因果推理；会“像真的”，不保证“是真的”。

长期记忆与自我一致性：上下文窗再大，也没有稳定的自传体记忆与人格一致性。

幻觉与事实一致性：RAG能缓解，不能根治；缺少内置“事实验证器”。

价值判断与责任：有“对齐”无“动机”；不能承担伦理与法律责任。

可解释性与鲁棒性：推理链条不可验；对抗扰动/分布外数据表现脆弱。

具身智能与因果控制：在真实物理世界，样本效率低、仿真到现实（sim2real）困难。


AI = 统计近似 + 工程增强，不是通用智能。
大模型把交互做到了“像人”，但尚未跨过因果、记忆、动机三道门。


在通用智能的主战场，Transformer 仍是王者，没有被替代；但在效率和特定任务上，各类新架构都在补位。未来很可能是“Transformer 为主、多架构混合”的局面。

## GPT - transformer
Transformer = 当前AI的绝对核心

语言模型、图像模型、甚至扩散模型，几乎全靠它。

强在能处理超长上下文，能并行算，适合大规模堆GPU。

替代路线在冒头

RNN复兴（RWKV、Mamba）：更省算力，尝试解决Transformer“烧钱”的问题。

MoE（专家混合）：不是新架构，是把参数稀疏化，提升效率。

Diffusion扩散模型：主要在图像/音频，跟Transformer互补，现在也常被Transformer混合改造。

趋势

短期内：Transformer一家独大。

长期看：可能出现“混合架构”，不同任务用不同最佳工具。